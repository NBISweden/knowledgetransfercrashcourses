""" Snakefile with rules to run the Slamdunk workflow."""

# module import
""" Here we import a python module that we need for the following two
   python functions """
import os

# Two python functions that just allow simpler notation in rules
""" It is good keep the code and the results separate, so
    we run the workflow in a separate working directory. However,
    we sometimes need to access files in the code folder. This
    function prepends a file path with the path to the workflow
    base directory (kept in snakemake variable workflow.basedir) """
def prependWfd(path):
    return(os.path.normpath(os.path.join(workflow.basedir, path)))

""" Mostly, snakemake uses relative paths (to the working directory)
    when referring to, e.g., input and output files. However, when
    soft-linking (creating aliases) from external file, full paths
    must be used. This function prepends the path to the current
    working directory (accessed by the python function os.getcwd()). """
def prependPwd(path):
    return(os.path.normpath(os.path.join(os.getcwd(), path)))

# Configure files
""" Read the yaml config file in the pwd. If not existing, warn user and
    copy the one from the workflow source directory. """
if not os.path.exists("SlamdunkConfig.yaml"):
    print("No user-defined config file, SlamdunkConfig.yaml, found in "
          "the present directory. Copying the config file from the workflow"
          "source directory. You probably want to edit this and add your own changes.")
    os.system("cp {} SlamdunkConfig.yaml".format(prependWfd('SlamdunkConfig.yaml')))
configfile: "SlamdunkConfig.yaml"

""" read sample info from tabular config file 'samples.tsv'. This can be a
    tab-separated or comma-separated spreadsheet text file. If not existing,
    warn user and copy the one from the workflow source directory.
    columns can be accessed as 'samples['<colname>']' and cells canbe accessed
    as 'samples['<col>']['<sample name>']'. """
import pandas as pd
if not os.path.exists("SlamdunkSamples.tsv"):
    print("No user-defined tabular config file, SlamdunkSamples.yaml, found in\n"
          "the present directory. Copying the config file from the workflow \n"
          "source directory. You probably want to edit this and add your own changes.")
    os.system("cp {} SlamdunkSamples.tsv".format(prependWfd('SlamdunkSamples.tsv')))
samples = pd.read_csv("SlamdunkSamples.tsv", sep="[\t,]",
                      comment="#", engine="python").set_index("sample")

# Cluster, groups, and localrules
""" The rules that softlinks external files) are ridiculously fast;
    run these on the login node by listing them as 'localrules'. """
localrules: linkFastq, linkReference, linkRegions


# Target rule
""" By listing the final count files for all samples, this will run the
    whole workflow for all samples """
rule all:
    input:
        # expand creates a list by replacing {sample} by all
        # sample names in samples
        count = expand(
            "Slamdunk/count/{sample}.FASTQ_slamdunk_mapped_filtered_tcount.tsv",
            sample = samples.index
        )

# Rule order
""" The remaining rules are written in the order of expected execution"""

# It is good practice to softlink all external files into the working directory
rule linkFastq:
    output:
        fastq = "Slamdunk/fastq/{sample}.FASTQ.gz"
    input:
        # see further SlamdunkConfig.yaml
        fastq = lambda wc: samples["fastq"][wc.sample]
    params:
        # full path needed for ln -s, neater to redefine in params.
        fastq = lambda wc: prependPwd("Slamdunk/fastq/{s}.FASTQ.gz".format(s=wc.sample))
    log:
        # must have (all) same wildcards as output
        log = "Slamdunk/log/linkFastq_{sample}.log"
    shell:
        """
        # everything inside curly braces is expanded to the corresponding variable defined above,
        # e.g., {log.log} becomes "Slamdunk/log/linkFastq_<theactualSampleName>.log"
        exec &> {log.log}

        echo "Linking {input.fastq} to {output.fastq}"
        ln -s {input.fastq} {params.fastq}
        echo "Done!"
        """

rule linkReference:
    output:
        reference = "Slamdunk/metadata/reference.fasta"
    input:
        reference = config["reference"]
    params:
        reference = prependPwd("Slamdunk/metadata/reference.fasta")
    log:
        log = "Slamdunk/log/linkReference.log"
    shell:
        """
        exec &> {log.log}

        echo "Linking {input.reference} to {output.reference}"
        ln -s {input.reference} {params.reference}
        echo "Done!"
        """

rule linkRegions:
    output:
        regions = "Slamdunk/metadata/regions.bed",
    input:
        regions = config["regions"]
    params:
        regions = prependPwd("Slamdunk/metadata/regions.bed"),
    log:
        log = "Slamdunk/log/linkRegions.log"
    shell:
        """
        exec &> {log.log}

        echo "Linking {input.regions} to {output.regions}"
        ln -s {input.regions} {params.regions}
        echo "Done!"
        """

rule map:
    output:
        bam = "Slamdunk/mapped/{sample}.FASTQ_slamdunk_mapped.bam",
    input:
        reference = "Slamdunk/metadata/reference.fasta",
        fastq = "Slamdunk/fastq/{sample}.FASTQ.gz"
    params:
        outdir = "Slamdunk/mapped"
    # The conda directive points to a conda environment file (see documentation in that file)
    # If snakemake is run with option `--use-conda`, it will create this conda environment
    # and run this rule within that environment.
    conda: "envs/slamdunk.yaml"
    # The group directive enables sending several rules as on cluster job (deafult is one
    # rule: one job). rules with the same group name will be run in the same job.
    group: "mapAndFilter"
    log:
        log = "Slamdunk/log/map_{sample}.log"
    shell:
        """
        exec &> {log.log}

        echo "Running slamdunk version:"
        slamdunk --version
        echo "to perform dunk map on {input.fastq}"
        slamdunk map -r {input.reference} -o {params.outdir} -t 1 {input.fastq}

        # additional possible options:
        # -i {{params.index}}
        # -5 <bp to trim from 5' end>
        # -n <Output up to N alignments per multimapper>
        # -a <maximum number of 3' As before trimming
        # -q -e -ss
        ls {params.outdir}
        echo "Done"
        """

rule index:
    output:
        bai = "Slamdunk/mapped/{prefix}.bam.bai"
    input:
        bam = "Slamdunk/mapped/{prefix}.bam",
    params:
        tmp = "Slamdunk/mapped/{prefix}.bam.bkp"
    conda: "envs/samtools.yaml"
    group: "mapAndFilter"
    log:
        log = "Slamdunk/log/index_{prefix}.log"
    shell:
        """
        exec &> {log.log}
        echo "Create backup"
        mv {input.bam} {params.tmp}

        echo "Sorting and indexing bam"
    	samtools sort {params.tmp} -o {input.bam}
        samtools index {input.bam}

        # remove backup
        rm -f {params.tmp}
        echo "Done"
        """

rule filter:
    output:
        bam = "Slamdunk/filtered/{sample}.FASTQ_slamdunk_mapped_filtered.bam",
        bai = "Slamdunk/filtered/{sample}.FASTQ_slamdunk_mapped_filtered.bam.bai"
    input:
        regions = "Slamdunk/metadata/regions.bed",
        bam = "Slamdunk/mapped/{sample}.FASTQ_slamdunk_mapped.bam",
        bai = "Slamdunk/mapped/{sample}.FASTQ_slamdunk_mapped.bam.bai"
    params:
        outdir = "Slamdunk/filtered"
    conda: "envs/slamdunk.yaml"
    group: "mapAndFilter"
    log:
        log = "Slamdunk/log/filter_{sample}.log"
    shell:
        """
        exec &> {log.log}

        echo "Running slamdunk version:"
        slamdunk --version
        echo "to perform dunk filter on {input.bam}"
        slamdunk filter -o {params.outdir} -b {input.regions} -t 1 {input.bam}
        # additional possible options:
        # -mq <MQ cutoff>
        # -mi <identity cutoff>
        # -nm <NM cutoff>

        echo "Done"
        """

rule snp:
    output:
        snp = "Slamdunk/snp/{sample}_slamdunk_mapped_filtered_snp.vcf"
    input:
        reference = "Slamdunk/metadata/reference.fasta",
        bam = "Slamdunk/filtered/{sample}_slamdunk_mapped_filtered.bam",
        bai = "Slamdunk/filtered/{sample}_slamdunk_mapped_filtered.bam.bai"
    params:
        outdir = "Slamdunk/snp"
    conda: "envs/slamdunk.yaml"
    group: "snpAndCount"
    log:
        log = "Slamdunk/log/snp_{sample}.log"
    shell:
        """
        exec &> {log.log}

        echo "Running slamdunk version:"
        slamdunk --version
        echo "to perform dunk snp on {input.bam}"
        slamdunk snp -o {params.outdir} -r {input.reference}  -t 2 {input.bam}
        # additional possible options:
        # -c {{params.coverage}}
        # -f {{params.variant_frac}}

        echo "Done"
        """

rule count:
    output:
        tcount = "Slamdunk/count/{sample}.FASTQ_slamdunk_mapped_filtered_tcount.tsv"
    input:
        snp = "Slamdunk/snp/{sample}.FASTQ_slamdunk_mapped_filtered_snp.vcf",
        reference = "Slamdunk/metadata/reference.fasta",
        regions = "Slamdunk/metadata/regions.bed",
        bam = "Slamdunk/filtered/{sample}.FASTQ_slamdunk_mapped_filtered.bam",
        bai = "Slamdunk/filtered/{sample}.FASTQ_slamdunk_mapped_filtered.bam.bai"
    params:
        outdir = "Slamdunk/count/",
        snpdir = "Slamdunk/snp/",
        readlength = lambda wc: samples["readlength"][wc.sample]
    conda: "envs/slamdunk.yaml"
    group: "snpAndCount"
    log:
        log = "Slamdunk/log/count_{sample}.log"
    shell:
        """
        exec &> {log.log}

        echo "Running slamdunk version:"
        slamdunk --version
        echo "to perform dunk count on {input.bam}"
        time slamdunk count -o {params.outdir} -s {params.snpdir}  -r {input.reference} -b {input.regions} -t 2 {input.bam} -l {params.readlength}
        # additional possible options:
        # -m
        # -q <minimum base quality>

        echo "Done"
        """
